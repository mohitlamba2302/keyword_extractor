{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stop + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ModPosTag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def simple_clean(words):\n",
    "#     output_words = [w.lower() for w in words if not w.lower() in stop]\n",
    "#     return output_words\n",
    "\n",
    "def simple_clean(words):\n",
    "    output_words = [w for w in words if not w in stop]\n",
    "    return output_words\n",
    "\n",
    "\n",
    "# def complex_clean(words):\n",
    "#     output_words = []\n",
    "#     for w in words:\n",
    "#         if w.lower() not in stop:\n",
    "#             pos = pos_tag([w])\n",
    "#             clean_w = lemmatizer.lemmatize(w, pos = ModPosTag(pos[0][1]))\n",
    "#             output_words.append(clean_w.lower())\n",
    "#     return output_words\n",
    "\n",
    "# remove lemetize\n",
    "def complex_clean(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w not in stop:\n",
    "            pos = pos_tag([w])\n",
    "            clean_w = lemmatizer.lemmatize(w, pos = ModPosTag(pos[0][1]))\n",
    "            output_words.append(clean_w)\n",
    "    return output_words\n",
    "\n",
    "\n",
    "def read_words(words_dir):\n",
    "\n",
    "    files = [os.path.join(words_dir, fi) for fi in os.listdir(words_dir)]\n",
    "\n",
    "    #features_matrix = np.zeros((len(files), 3000))\n",
    "    docID = 0;\n",
    "    all_words = []\n",
    "    all_words_heading = []\n",
    "    major_words_abstract = []\n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            words_abstract = []\n",
    "            for i, line in enumerate(fi):\n",
    "                if (i == 0):\n",
    "                    temp = line.split()\n",
    "                    temp1 = simple_clean(temp)\n",
    "                    \n",
    "                    words_abstract += temp1\n",
    "                    all_words_heading += temp1\n",
    "                else:\n",
    "                    temp = line.split()\n",
    "                    temp1 = complex_clean(temp)\n",
    "                    print(fil)\n",
    "                    print()\n",
    "                    print(temp1)\n",
    "                    words_abstract += temp1\n",
    "                    all_words += temp1\n",
    "\n",
    "            major_words_abstract.append(words_abstract)\n",
    "            len(all_words)\n",
    "    return major_words_abstract, all_words, all_words_heading\n",
    "\n",
    "def tag_updater(df, variable_code):\n",
    "    for key, values in variable_code.items():\n",
    "        temp = str(key)\n",
    "        area = variable_code[key]\n",
    "        for val in area:\n",
    "            df.at[val-1, temp] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_tags(words_dir):\n",
    "    files = [os.path.join(words_dir, fi) for fi in os.listdir(words_dir)]\n",
    "\n",
    "    # features_matrix = np.zeros((len(files), 3000))\n",
    "    docID = 0;\n",
    "    tagss = set()\n",
    "    for fil in files:\n",
    "        docID+=1\n",
    "        with open(fil) as fi:\n",
    "            for i, line in enumerate(fi):\n",
    "                temp = line.split(',')\n",
    "                tagss.update(temp)\n",
    "    return docID, tagss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {}\n",
    "\n",
    "def read_files(words_directory, tags_directory):\n",
    "\n",
    "    wo = [os.path.join(words_directory, wi) for wi in os.listdir(words_directory)]\n",
    "    ta = [os.path.join(tags_directory, ti) for ti in os.listdir(tags_directory)]\n",
    "    ctr=0\n",
    "    for (ab, t) in zip(wo, ta):\n",
    "        ctr +=1\n",
    "        with open(t) as su:\n",
    "            for i, line in enumerate(su):\n",
    "                tag = line.split(',')\n",
    "                for q in tag:\n",
    "                    if q not in mydict:\n",
    "                        mydict[q] = []\n",
    "                    mydict[q].append(ctr)\n",
    "\n",
    "    return mydict,ctr\n",
    "\n",
    "def extract_features(Words, features):\n",
    "    feature_matrix = np.zeros((len(Words), len(features)))\n",
    "    docID = 0\n",
    "    for doc in Words:\n",
    "        for word in doc:\n",
    "            for i in range(len(features)):\n",
    "                if features[i] == word:\n",
    "                    wordID = i\n",
    "                    feature_matrix[docID, wordID] += 1\n",
    "        docID = docID + 1\n",
    "    np.shape(feature_matrix)\n",
    "    print(feature_matrix)\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dir = 'abstract'\n",
    "tags_directory = 'tags'\n",
    "Words, all_words, all_words_heading = read_words(words_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "cou, Tags = read_tags(tags_directory)\n",
    "#df = pd.DataFrame(Tags)\n",
    "Words = np.array(Words)\n",
    "# exploring frequency of all words not in heading\n",
    "import nltk\n",
    "freq = nltk.FreqDist(all_words + all_words_heading)\n",
    "common = freq.most_common(3000)\n",
    "common = list(common)\n",
    "\n",
    "features = []\n",
    "features += [w[0] for w in common]\n",
    "features += [w for w in all_words_heading if w not in common]\n",
    "print(len(common))\n",
    "print(len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DS</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Machine Learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DS   Algorithm   AI  Machine Learning\n",
       "0    1           1    0                 0\n",
       "1    1           1    0                 0\n",
       "2    1           1    0                 0\n",
       "3    1           1    0                 0\n",
       "4    1           1    0                 0\n",
       "5    1           1    0                 0\n",
       "6    1           1    0                 0\n",
       "7    1           1    0                 0\n",
       "8    1           1    0                 0\n",
       "9    1           1    0                 0\n",
       "10   0           0    1                 1\n",
       "11   1           1    0                 0\n",
       "12   1           1    0                 0\n",
       "13   1           1    0                 0\n",
       "14   1           1    0                 0\n",
       "15   1           1    0                 0\n",
       "16   1           1    0                 0\n",
       "17   1           1    0                 0\n",
       "18   1           1    0                 0\n",
       "19   0           0    1                 1\n",
       "20   1           1    0                 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "        require_dense=[True, True])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "variable_code, ctr = read_files(words_dir, tags_directory)\n",
    "columns = list(Tags)\n",
    "rows = list(range(ctr))\n",
    "dataFrame = np.zeros((len(rows), len(columns)))\n",
    "#df = tag_updater(dataFrame, variable_code, columns)\n",
    "\n",
    "df = pd.DataFrame(data = dataFrame, index = rows, columns = columns, dtype='int64')\n",
    "# print(variable_code)\n",
    "Y = tag_updater(df, variable_code)\n",
    "#print(df.at[1, ' Machine learning '])\n",
    "# for x in df['Computers']:\n",
    "#     print(x)\n",
    "\n",
    "display(Y)\n",
    "Y.fillna(0, inplace=True)\n",
    "\n",
    "X = extract_features(Words, features)\n",
    "X_train = pd.DataFrame(data = X, index = rows, columns = features, dtype='int64')\n",
    "\n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "# train\n",
    "classifier.fit(X_train, Y)\n",
    "# predict\n",
    "#predictions = classifier.predict(X_train)\n",
    "#print(predictions.toarray())\n",
    "#print(accuracy_score(Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      "  1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "words_dir = 'testing'\n",
    "Words, waste1, waste2 = read_words(words_dir)\n",
    "X = extract_features(Words, features)\n",
    "rows = list(range(1))\n",
    "X_test = pd.DataFrame(data = X, columns = features, dtype='int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Support</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Machines</th>\n",
       "      <th>Classify</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Systems</th>\n",
       "      <th>Support</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Machines</th>\n",
       "      <th>Classify</th>\n",
       "      <th>...</th>\n",
       "      <th>Machines</th>\n",
       "      <th>Classify</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Systems</th>\n",
       "      <th>Support</th>\n",
       "      <th>Vector</th>\n",
       "      <th>Machines</th>\n",
       "      <th>Classify</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Systems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Support  Vector  Machines  Classify  Learning  Systems  Support  Vector  \\\n",
       "0        1       1         1         0         0        0        1       1   \n",
       "\n",
       "   Machines  Classify   ...     Machines  Classify  Learning  Systems  \\\n",
       "0         1         0   ...            1         0         0        0   \n",
       "\n",
       "   Support  Vector  Machines  Classify  Learning  Systems  \n",
       "0        1       1         1         0         0        0  \n",
       "\n",
       "[1 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:436: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:436: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:436: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:436: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n",
      "C:\\Users\\Mohit\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:438: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    }
   ],
   "source": [
    "display(X_test)\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Column format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(predictions)\n",
    "predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "print(type(predictions))\n",
    "a = predictions.nonzero()\n",
    "#a.row[a.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(self):\n",
    "    A = self.tocoo()\n",
    "    nz_mask = A.data != 0\n",
    "    return (list(A.col[nz_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "[[0. 0. 0. 0.]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print((predictions.shape[0], len(columns)))\n",
    "dataFrame5 = np.zeros((predictions.shape[0], len(columns)))\n",
    "ct = 0\n",
    "for i in predictions:\n",
    "    b = find_index(i)\n",
    "    for j in b:\n",
    "        dataFrame5[ct,j] +=1\n",
    "    ct+=1\n",
    "print(dataFrame5)\n",
    "for i in dataFrame5:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DS</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Machine Learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DS   Algorithm   AI  Machine Learning\n",
       "0   0           0    0                 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(dataFrame))\n",
    "\n",
    "converting = pd.DataFrame(data = dataFrame5, columns = columns, dtype='int64')\n",
    "display(converting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file_predicted_skills.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------TECHNICAL SKILLS REQUIRED-----------------------------\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "we_have = []\n",
    "print('------------------------TECHNICAL SKILLS REQUIRED-----------------------------')\n",
    "for i in b:\n",
    "    print(columns[i])\n",
    "    we_have.append(columns[i])\n",
    "    \n",
    "#we_have.append('artificial intelligence')\n",
    "#we_have.append('Machine learning algorithms')\n",
    "#we_have.append('Support vector machines')\n",
    "#we_have.append('Kernel')\n",
    "\n",
    "print(we_have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is keyword extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next comes the wiki reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identifying and seizing fast breaking opportunities', 'evaluation of commercial opportunities', 'identification and evaluation of market opportunities', 'opportunity management'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def load_skill_set():\n",
    "\n",
    "    skill_dict = dict()\n",
    "    pick_file = open(\"final_skill_dict.pickle\", \"rb\")\n",
    "    skill_dict = pickle.load(pick_file)\n",
    "    pick_file.close()\n",
    "    return skill_dict\n",
    "\n",
    "\n",
    "skill_dict = load_skill_set()\n",
    "\n",
    "\n",
    "def get_skills(skill_dic,topic):\n",
    "\n",
    "    # print(\"getting skill set for \",topic)\n",
    "    skill_set = set()\n",
    "    for word in topic.split(' '):\n",
    "        # print(\"for word=\",word)\n",
    "        if word.strip().isalpha():\n",
    "            word = word.strip().lower()\n",
    "            wordnet_pos = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "            if wordnet_pos == '':\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            else:\n",
    "                word = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "\n",
    "            try:\n",
    "                curr_skill_set = skill_dic[word]\n",
    "                # print(\"skill set=\",curr_skill_set)\n",
    "                if len(skill_set) == 0:\n",
    "                    skill_set = curr_skill_set\n",
    "                else:\n",
    "                    skill_set = skill_set.intersection(curr_skill_set)\n",
    "\n",
    "                #print(\"intersection=\",skill_set)\n",
    "            except KeyError as e:\n",
    "                print(\"no skill set found\",e)\n",
    "\n",
    "    return skill_set\n",
    "\n",
    "\n",
    "print(get_skills(skill_dict,'opportunity'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page(topic):\n",
    "    domain = \"https://en.wikipedia.org\"\n",
    "    html = urllib.request.urlopen(\"https://en.wikipedia.org/w/index.php?search=\"+topic.replace(' ','+')+\"&title=Special%3ASearch&go=Go\")\n",
    "    soup = BeautifulSoup(html, features=\"lxml\")\n",
    "    first_result = soup.find(attrs={\"data-serp-pos\": \"0\"})\n",
    "    if first_result is None:\n",
    "        print('page-found')\n",
    "        return soup\n",
    "    href = first_result.get('href')\n",
    "    print('opening first-result')\n",
    "    html = urllib.request.urlopen(domain+href)\n",
    "    soup = BeautifulSoup(html, features=\"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_first_para(topic):\n",
    "    soup = get_page(topic)\n",
    "    text_section = soup.find(attrs={'class': 'mw-parser-output'})\n",
    "    text = ''\n",
    "    for child in text_section.children:\n",
    "        # print('for tag', child.name, child)\n",
    "        try:\n",
    "            if child is not None:\n",
    "                if child.name == 'p':\n",
    "                    text += child.text.lower()\n",
    "                elif child.name == 'div' and 'toc' in child['class']:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(\"exception\", e)\n",
    "    return text\n",
    "\n",
    "vo_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# cleaning we _have \n",
    "dore = []\n",
    "for x in we_have:\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    #First parameter is the replacement, second parameter is your input string\n",
    "    #regex.sub('', x)\n",
    "    dore.append(re.sub(\"[^a-zA-Z_ ]*\", \"\", x))\n",
    "\n",
    "for y in dore:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('tech2.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(dore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #print(get_first_para('computer vision'))\n",
    "    for original in dore: \n",
    "        print(original)\n",
    "        w1=get_first_para(original)\n",
    "        w2=word_tokenize(w1)\n",
    "        \n",
    "        print(w1)\n",
    "\n",
    "        for word in w2:                                                                 \n",
    "            if word not in stop and word.__len__()>3 and word.isalpha():\n",
    "                w3 = get_skills(skill_dict, word)\n",
    "                for x in w3:\n",
    "                    vo_set.add(x)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facilitation skills\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def load_skill_set2():\n",
    "\n",
    "    skill_dict = dict()\n",
    "    pick_file = open(\"final_skill_dict_2.pickle\", \"rb\")\n",
    "    skill_dict = pickle.load(pick_file)\n",
    "    pick_file.close()\n",
    "    return skill_dict\n",
    "\n",
    "\n",
    "def get_skill2(skill_dic,topic):\n",
    "    return skill_dic[ lemmatizer.lemmatize(topic.strip()) ]\n",
    "\n",
    "\n",
    "# skill_dict = make_skill_dict()\n",
    "skill_dict2 = load_skill_set2()\n",
    "\n",
    "print(get_skill2(skill_dict2,'procurement management'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(vo_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = set()\n",
    "for i in vo_set:\n",
    "    if i in skill_dict2:\n",
    "        new_set.add(get_skill2(skill_dict2,i))\n",
    "\n",
    "len(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------NON- TECHNICAL SKILLS---------------\n",
      "communication skills\n",
      "scope management\n",
      "emotional intelligence\n",
      "marketing communications\n",
      "sales pipeline management\n",
      "negotiation\n",
      "technical skills\n",
      "cultural competence\n",
      "sales operations\n",
      "visual abilities\n",
      "core project management skills\n",
      "benefits management\n",
      "quality management\n",
      "procurement management\n",
      "speaking skills\n",
      "sales management\n",
      "information modeling & design\n",
      "selling skills\n",
      "networking\n",
      "program management skills\n",
      "core executive leadership skills\n",
      "basic leadership skills\n",
      "risk management\n",
      "executive management\n",
      "core management skills\n",
      "creative skills\n",
      "problem solving\n",
      "time management\n",
      "entrepreneurial skills\n",
      "technology (for it project management)\n",
      "idea formation\n",
      "program lifecycle\n",
      "management\n",
      "market research\n",
      "targeted communication\n",
      "human resources management\n",
      "sales strategy\n",
      "brand management\n",
      "writing\n",
      "persuasion techniques\n",
      "influencing to negotiate\n",
      "using emotions\n",
      "decision making\n",
      "hard bargaining\n",
      "sale\n",
      "influencing\n",
      "human resources\n",
      "managerial finance\n",
      "finance\n",
      "information visualization\n",
      "presentation skills\n",
      "negotiation skills\n",
      "soft skills\n",
      "time management methods\n",
      "stakeholder management\n",
      "marketing strategy & planning\n",
      "conflict resolution\n",
      "persuasion\n",
      "negotiating\n",
      "project\n",
      "management focus areas\n",
      "business execution\n",
      "organizational behavior\n",
      "artistic abilities\n",
      "interpersonal skills\n",
      "stage presence\n",
      "managing teams\n",
      "marketing approaches\n",
      "personal skills\n",
      "governance\n",
      "communication\n",
      "core skills\n",
      "facilitation skills\n",
      "business development\n",
      "promotion\n",
      "integration management\n",
      "strategy\n",
      "change management\n",
      "presenting visual information\n",
      "cost management\n",
      "observing & analyzing\n",
      "business & product development\n",
      "strategic program management\n",
      "delivery\n",
      "applying innovation techniques\n",
      "establishing rapport\n",
      "marketing\n",
      "technology\n",
      "mergers, acquisitions & divestiture\n",
      "visual communication\n",
      "professional skills\n",
      "innovation\n",
      "selling\n",
      "diplomacy\n",
      "international business\n",
      "leadership\n",
      "complex negotiations\n"
     ]
    }
   ],
   "source": [
    "print('--------------NON- TECHNICAL SKILLS---------------')\n",
    "for word in new_set:\n",
    "    print(word)\n",
    "my_list2 = list(new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('nontech2.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(my_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying out reading just the extract for the technical skills\n",
    "import os\n",
    "files = [os.path.join('testing', fi) for fi in os.listdir('testing')]\n",
    "my_text = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for fil in files:\n",
    "    with open(fil) as fi:\n",
    "            line = fi.read()\n",
    "            temp= word_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2 = list(temp)\n",
    "my_text =  [w.lower() for w in temp2 if not w.lower() in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['determination',\n",
       " 'vocational',\n",
       " 'fields',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'importance',\n",
       " 'vocational',\n",
       " 'technical',\n",
       " 'training',\n",
       " 'growing',\n",
       " 'day',\n",
       " 'day',\n",
       " 'parallel',\n",
       " 'developing',\n",
       " 'technology',\n",
       " 'inevitable',\n",
       " 'utilise',\n",
       " 'opportunities',\n",
       " 'presented',\n",
       " 'information',\n",
       " 'communication',\n",
       " 'technologies',\n",
       " 'order',\n",
       " 'determine',\n",
       " 'vocational',\n",
       " 'fields',\n",
       " 'vocational',\n",
       " 'technical',\n",
       " 'training',\n",
       " 'efficient',\n",
       " 'manner',\n",
       " 'respect',\n",
       " 'possible',\n",
       " 'create',\n",
       " 'efficient',\n",
       " 'tool',\n",
       " 'compared',\n",
       " 'current',\n",
       " 'methods',\n",
       " 'utilising',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'model',\n",
       " 'energy',\n",
       " 'applications',\n",
       " 'predicts',\n",
       " 'events',\n",
       " 'future',\n",
       " 'depending',\n",
       " 'past',\n",
       " 'experiences',\n",
       " 'current',\n",
       " 'study',\n",
       " 'software',\n",
       " 'developed',\n",
       " 'ensures',\n",
       " 'system',\n",
       " 'learns',\n",
       " 'successful',\n",
       " 'unsuccessful',\n",
       " 'choices',\n",
       " 'made',\n",
       " 'past',\n",
       " 'applying',\n",
       " '“',\n",
       " 'naive',\n",
       " 'bayes',\n",
       " '”',\n",
       " 'algorithm',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'data',\n",
       " 'collected',\n",
       " 'concerning',\n",
       " 'individuals',\n",
       " 'turned',\n",
       " 'successful',\n",
       " 'unsuccessful',\n",
       " 'vocational',\n",
       " 'technical',\n",
       " 'training',\n",
       " 'process',\n",
       " 'energy',\n",
       " 'applications',\n",
       " 'software',\n",
       " 'developed',\n",
       " 'aimed',\n",
       " 'system',\n",
       " 'recommends',\n",
       " 'suitable',\n",
       " 'vocational',\n",
       " 'field',\n",
       " 'individual',\n",
       " 'according',\n",
       " 'data',\n",
       " 'collected',\n",
       " 'individual',\n",
       " 'occupation',\n",
       " 'selection',\n",
       " 'process',\n",
       " 'field',\n",
       " 'energy',\n",
       " 'applications']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_set = set()\n",
    "for q in my_text:\n",
    "    if q in skill_dict:\n",
    "        veu = get_skills(skill_dict,q)\n",
    "        for qw in veu:\n",
    "            q_set.add(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "communication skills\n",
      "emotional intelligence\n",
      "creative direction\n",
      "digital business strategy\n",
      "communications to stakeholders\n",
      "managing emotions\n",
      "applying economic models\n",
      "listening.\n",
      "visual expression of concepts\n",
      "publicity\n",
      "marketing communications\n",
      "intuition\n",
      "creative questioning\n",
      "flow charts\n",
      "graphs\n",
      "creating and maintaining a high performance culture\n",
      "project metric development and communications\n",
      "effective greetings\n",
      "friendliness\n",
      "technology savvy\n",
      "interviewing\n",
      "whiteboarding\n",
      "communications\n",
      "targeting information to an audience\n",
      "statistical analysis\n",
      "negotiation\n",
      "empathy\n",
      "developing presentations\n",
      "timelines\n",
      "using project management tools\n",
      "designing graphics\n",
      "community engagement\n",
      "quick-wittedness\n",
      "reporting & communication\n",
      "communications to executive leadership\n",
      "communication plans\n",
      "cultural competence\n",
      "managing a portfolio of it investments\n",
      "research\n",
      "applying data to decisions\n",
      "managing training\n",
      "visualization standards\n",
      "fluid intelligence\n",
      "marketing intelligence management\n",
      "selective listening\n",
      "technology trend awareness\n",
      "manage customer relationship management (crm) information\n",
      "choice modeling\n",
      "it standards & best practices (e.g. itil)\n",
      "facilitating meetings\n",
      "posture\n",
      "conceptual blending\n",
      "business communication\n",
      "managing communications to the board of directors\n",
      "creating brand ambassadors\n",
      "training\n",
      "current state analysis\n",
      "brainstorming\n",
      "risk management tools (e.g. risk databases)\n",
      "management of sales training and development programs\n",
      "presentation flow\n",
      "customer relationships\n",
      "active listening\n",
      "developing strategic profit models\n",
      "media selection and planning\n",
      "sales information & technology strategy\n",
      "information privacy\n",
      "computer-aided design\n",
      "developing metrics & kpi\n",
      "self control\n",
      "technical assessments\n",
      "the ability to influence an audience.\n",
      "listening skills\n",
      "information analysis\n",
      "technical drawings\n",
      "language abilities\n",
      "process improvement\n",
      "targeted communications\n",
      "reputation management\n",
      "motivating\n",
      "perceiving emotions\n",
      "listening\n",
      "using time management tools\n",
      "unbiased thinking\n",
      "active silence\n",
      "control of the voice (intonation & inflection)\n",
      "body language & facial expressions\n",
      "coaching\n",
      "time management\n",
      "executive communication\n",
      "trend awareness\n",
      "strategic management of technology\n",
      "understanding emotions\n",
      "business model development\n",
      "targeted communication\n",
      "ability to take criticism\n",
      "written and visual communication\n",
      "speaking rhythm\n",
      "data transformations\n",
      "business analysis\n",
      "energy\n",
      "design sense\n",
      "infonomics\n",
      "visual presentation\n",
      "writing\n",
      "physical communication\n",
      "self-presentation\n",
      "software development lifecycle\n",
      "architecture management\n",
      "technology scope creep management (feature creep)\n",
      "effective use of microphone and sound system\n",
      "treemaping\n",
      "price models\n",
      "presentations to hostile audiences\n",
      "humor\n",
      "articulate speech\n",
      "physical communication & body language\n",
      "using emotions\n",
      "language ability\n",
      "visual communication design\n",
      "persuasion.\n",
      "social skills\n",
      "verbal & visual communication\n",
      "influencing\n",
      "self awareness\n",
      "creating a high performance culture\n",
      "funding models\n",
      "public speaking & presentation skills\n",
      "integrated marketing communications\n",
      "developing communication plans\n",
      "simplifying complex ideas with visual representations\n",
      "order fulfillment management\n",
      "advertising\n",
      "concept maps\n",
      "attunement to social norms\n",
      "simplifying complex ideas\n",
      "data flow diagrams\n",
      "presentation skills\n",
      "intelligence\n",
      "using silence\n",
      "argument maps\n",
      "status communication\n",
      "statistical graphics\n",
      "governance processes and procedures\n",
      "first impressions\n",
      "validates relevance\n",
      "modeling decisions\n",
      "heatmaps\n",
      "sapience\n",
      "branding\n",
      "mindmaps\n",
      "client and stakeholder relationship management\n",
      "taking criticism\n",
      "communications to customers\n",
      "stakeholder analysis\n",
      "managing change processes\n",
      "fostering personal courage in others\n",
      "observing & analyzing emotions\n",
      "information architecture\n",
      "verbal communication\n",
      "strategic analysis of accounting information\n",
      "using visual display technologies\n",
      "public speaking\n",
      "facial expressions\n",
      "preparing executive summaries\n",
      "sales communications\n",
      "benchmarking\n",
      "plan sales training & development\n",
      "interpersonal skills\n",
      "using change management tools\n",
      "technical communication\n",
      "using visualization tools\n",
      "modeling information\n",
      "maps (cartography)\n",
      "financial closure processes\n",
      "multi-channel communications\n",
      "humor & quick-wittedness\n",
      "recognizing attempts to trigger emotional responses\n",
      "symbols\n",
      "marketing mix modeling\n",
      "managing stakeholder expectations\n",
      "supplier communication\n",
      "communication\n",
      "scientific diagrams\n",
      "ability to simplify complex ideas\n",
      "business writing (e.g. reports, emails)\n",
      "ability to analyze your own emotions\n",
      "art direction\n",
      "crisis communication\n",
      "intercultural competence\n",
      "self confidence\n",
      "business models\n",
      "customer service\n",
      "modeling business decisions\n",
      "mentoring\n",
      "project management information systems\n",
      "creating a sense of urgency and ownership\n",
      "applying data to decision making\n",
      "separating the people from the problem\n",
      "plain language (clear word choice that avoids jargon)\n",
      "storytelling\n",
      "software development methodologies\n",
      "applying humor to achieve business results.\n",
      "conversation\n",
      "effective presentation of self\n",
      "optimism\n",
      "charisma\n",
      "hierarchies\n",
      "drawing\n",
      "stress management\n",
      "it governance\n",
      "effective meetings\n",
      "program communications\n",
      "project status communications\n",
      "deflecting personal attacks\n",
      "endurance & persistence\n",
      "diagrams\n",
      "order management\n",
      "a fundamental aspect of communication in every culture.\n",
      "technology management\n",
      "financial analysis\n",
      "visual expression of emotion\n",
      "balanced scorecard\n",
      "corporate communication\n",
      "marketing\n",
      "network visualization (representing relationships)\n",
      "influencing without authority\n",
      "financial modeling\n",
      "visual communication\n",
      "strong eye contact\n",
      "selling\n",
      "modeling concepts\n",
      "body language\n",
      "governance models\n",
      "using accounting tools\n",
      "conducting feasibility studies\n",
      "critical questioning\n",
      "presentations\n",
      "finding order in chaos\n",
      "meeting management\n",
      "eye contact\n",
      "developing relationship audience\n",
      "effective word choice\n",
      "developing and reporting metrics and kpis\n",
      "using humor & wit\n",
      "developing strategic business models\n",
      "effective use of a whiteboard\n",
      "business process management\n",
      "knowledge management\n",
      "using sfa & crm tools\n",
      "mental sharpness and inventiveness\n",
      "engineering drawings\n",
      "gestures\n",
      "business process improvement\n",
      "social intelligence\n",
      "emotion management\n"
     ]
    }
   ],
   "source": [
    "for qw in q_set:\n",
    "    print(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qq_set = set()\n",
    "for i in q_set:\n",
    "    qq_set.add(get_skill2(skill_dict2,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for ele in qq_set:\n",
    "    temp = temp + (word_tokenize(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communication', 'skills', 'presenting', 'visual', 'information', 'cost', 'management', 'scope', 'management', 'emotional', 'intelligence', 'finance', 'information', 'visualization', 'marketing', 'communications', 'business', '&', 'product', 'development', 'strategic', 'program', 'management', 'core', 'executive', 'leadership', 'skills', 'managing', 'visual', 'information', 'sales', 'pipeline', 'management', 'risk', 'management', 'executive', 'management', 'core', 'management', 'skills', 'presentation', 'skills', 'soft', 'skills', 'applying', 'innovation', 'techniques', 'technical', 'skills', 'stakeholder', 'management', 'marketing', 'strategy', '&', 'planning', 'time', 'management', 'entrepreneurial', 'skills', 'project', 'reporting', '&', 'communication', 'technology', '(', 'for', 'it', 'project', 'management', ')', 'change', 'management', 'professional', 'communication', 'business', 'execution', 'cultural', 'competence', 'sales', 'operations', 'idea', 'formation', 'program', 'lifecycle', 'targeted', 'communication', 'market', 'research', 'marketing', 'management', 'technology', 'interpersonal', 'skills', 'visual', 'communication', 'professional', 'skills', 'human', 'resources', 'management', 'brand', 'management', 'sales', 'strategy', 'innovation', 'stage', 'presence', 'international', 'business', 'writing', 'procurement', 'management', 'persuasion', 'techniques', 'governance', 'leadership', 'selling', 'skills', 'operation', 'communication', 'sales', 'management', 'using', 'emotions', 'information', 'modeling', '&', 'design', 'decision', 'making', 'business', 'development', 'promotion', 'integration', 'management', 'strategy', 'leadership', 'of', 'visual', 'communication']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict123 = {}\n",
    "my_final = {}\n",
    "my_final['others'] = set()\n",
    "print(temp)\n",
    "for i in temp:\n",
    "    if i in dict123:\n",
    "        dict123[i] = dict123[i]+1\n",
    "    else:\n",
    "        dict123[i] = 1\n",
    "for ele in qq_set:\n",
    "    w = word_tokenize(ele)\n",
    "    for i in w:\n",
    "        if dict123[i]>5:\n",
    "            if i in my_final:\n",
    "                my_final[i].add(ele)\n",
    "            else:\n",
    "                my_final[i] = set()\n",
    "                my_final[i].add(ele)\n",
    "        if dict123[i]<2:\n",
    "            my_final['others'].add(ele)\n",
    "len(qq_set)\n",
    "len(my_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'communication': {'communication',\n",
       "  'communication skills',\n",
       "  'leadership of visual communication',\n",
       "  'professional communication',\n",
       "  'reporting & communication',\n",
       "  'targeted communication',\n",
       "  'visual communication'},\n",
       " 'management': {'brand management',\n",
       "  'change management',\n",
       "  'core management skills',\n",
       "  'cost management',\n",
       "  'executive management',\n",
       "  'human resources management',\n",
       "  'integration management',\n",
       "  'management',\n",
       "  'procurement management',\n",
       "  'risk management',\n",
       "  'sales management',\n",
       "  'sales pipeline management',\n",
       "  'scope management',\n",
       "  'stakeholder management',\n",
       "  'strategic program management',\n",
       "  'technology (for it project management)',\n",
       "  'time management'},\n",
       " 'others': {'applying innovation techniques',\n",
       "  'brand management',\n",
       "  'business & product development',\n",
       "  'business execution',\n",
       "  'change management',\n",
       "  'cost management',\n",
       "  'cultural competence',\n",
       "  'decision making',\n",
       "  'emotional intelligence',\n",
       "  'entrepreneurial skills',\n",
       "  'finance',\n",
       "  'governance',\n",
       "  'human resources management',\n",
       "  'idea formation',\n",
       "  'information modeling & design',\n",
       "  'information visualization',\n",
       "  'integration management',\n",
       "  'international business',\n",
       "  'interpersonal skills',\n",
       "  'leadership of visual communication',\n",
       "  'managing visual information',\n",
       "  'market research',\n",
       "  'marketing communications',\n",
       "  'marketing strategy & planning',\n",
       "  'operation',\n",
       "  'persuasion techniques',\n",
       "  'presentation skills',\n",
       "  'presenting visual information',\n",
       "  'procurement management',\n",
       "  'program lifecycle',\n",
       "  'promotion',\n",
       "  'reporting & communication',\n",
       "  'risk management',\n",
       "  'sales operations',\n",
       "  'sales pipeline management',\n",
       "  'scope management',\n",
       "  'selling skills',\n",
       "  'soft skills',\n",
       "  'stage presence',\n",
       "  'stakeholder management',\n",
       "  'strategic program management',\n",
       "  'targeted communication',\n",
       "  'technical skills',\n",
       "  'technology (for it project management)',\n",
       "  'time management',\n",
       "  'using emotions',\n",
       "  'writing'},\n",
       " 'skills': {'communication skills',\n",
       "  'core executive leadership skills',\n",
       "  'core management skills',\n",
       "  'entrepreneurial skills',\n",
       "  'interpersonal skills',\n",
       "  'presentation skills',\n",
       "  'professional skills',\n",
       "  'selling skills',\n",
       "  'soft skills',\n",
       "  'technical skills'}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### --------    Anupams dictionary  --------  ###\n",
    "\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import pickle\n",
    "# import xlrd\n",
    "\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # print(sheet.cell_value(0,0))\n",
    "\n",
    "\n",
    "# def make_skill_dict_final():\n",
    "\n",
    "#     file = (\"test_files/skills_anupam.xlsx\")\n",
    "\n",
    "#     wb = xlrd.open_workbook(file)\n",
    "#     sheet = wb.sheet_by_index(0)\n",
    "\n",
    "#     # print(len(lines))\n",
    "#     skill_dict = {}\n",
    "\n",
    "#     root_skill = ''\n",
    "#     for i in range(sheet.nrows):\n",
    "#         if sheet.cell_value(i,0) != '':\n",
    "#             root_skill = lemmatizer.lemmatize(sheet.cell_value(i, 0).strip().lower())\n",
    "\n",
    "#         for j in range(sheet.ncols):\n",
    "\n",
    "#             if sheet.cell_value(i, j) != '':\n",
    "#                 key = lemmatizer.lemmatize(sheet.cell_value(i, j).strip().lower())\n",
    "\n",
    "#                 if key in skill_dict:\n",
    "#                     skill_dict[key].add(root_skill)\n",
    "#                 else:\n",
    "#                     skill_dict[key] = {root_skill}\n",
    "\n",
    "#     print(\"printing dict.......\")\n",
    "#     for key in skill_dict:\n",
    "#         print(str(key).ljust(40) + str(skill_dict[key]).rjust(40))\n",
    "\n",
    "#     pick_file = open(\"anupam_skill_dict.pickle\",\"wb\")\n",
    "#     pickle.dump(skill_dict, pick_file)\n",
    "#     pick_file.close()\n",
    "\n",
    "#     return skill_dict\n",
    "\n",
    "\n",
    "# def load_skill_set_final():\n",
    "\n",
    "#     pick_file = open(\"anupam_skill_dict.pickle\", \"rb\")\n",
    "#     skill_dict = pickle.load(pick_file)\n",
    "#     pick_file.close()\n",
    "#     return skill_dict\n",
    "\n",
    "\n",
    "# def get_skill_final(skill_dic,topic):\n",
    "#     topic = lemmatizer.lemmatize(topic.strip().lower())\n",
    "#     if topic in skill_dic:\n",
    "#         return skill_dic[topic]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # skill_dict = make_skill_dict()\n",
    "# skill_dict_final = load_skill_set()\n",
    "\n",
    "# print(get_skill_final(skill_dict_final,'motivate'))\n",
    "# # while True:\n",
    "# #     inp = input(\"topic:\")\n",
    "# #     print(get_skill(skill_dict,inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Technical skills---------------\n",
      "Machine learning \n",
      " Machine learning algorithms \n",
      " Support vector machines \n",
      "\n",
      "----------------Technical skills---------------\n",
      "\n",
      "--------------------------------\n",
      "             -- others --\n",
      "---------------------------------\n",
      "presenting visual information ,\n",
      "cost management ,\n",
      "scope management ,\n",
      "emotional intelligence ,\n",
      "finance ,\n",
      "information visualization ,\n",
      "marketing communications ,\n",
      "business & product development ,\n",
      "strategic program management ,\n",
      "managing visual information ,\n",
      "sales pipeline management ,\n",
      "applying innovation techniques ,\n",
      "risk management ,\n",
      "presentation skills ,\n",
      "soft skills ,\n",
      "technical skills ,\n",
      "stakeholder management ,\n",
      "marketing strategy & planning ,\n",
      "time management ,\n",
      "entrepreneurial skills ,\n",
      "reporting & communication ,\n",
      "technology (for it project management) ,\n",
      "business execution ,\n",
      "cultural competence ,\n",
      "sales operations ,\n",
      "idea formation ,\n",
      "program lifecycle ,\n",
      "targeted communication ,\n",
      "market research ,\n",
      "interpersonal skills ,\n",
      "human resources management ,\n",
      "stage presence ,\n",
      "brand management ,\n",
      "international business ,\n",
      "writing ,\n",
      "procurement management ,\n",
      "persuasion techniques ,\n",
      "governance ,\n",
      "operation ,\n",
      "leadership of visual communication ,\n",
      "using emotions ,\n",
      "information modeling & design ,\n",
      "decision making ,\n",
      "promotion ,\n",
      "integration management ,\n",
      "selling skills ,\n",
      "change management ,\n",
      "--------------------------------\n",
      "             -- communication --\n",
      "---------------------------------\n",
      "communication skills ,\n",
      "targeted communication ,\n",
      "communication ,\n",
      "visual communication ,\n",
      "reporting & communication ,\n",
      "professional communication ,\n",
      "leadership of visual communication ,\n",
      "--------------------------------\n",
      "             -- skills --\n",
      "---------------------------------\n",
      "communication skills ,\n",
      "technical skills ,\n",
      "selling skills ,\n",
      "interpersonal skills ,\n",
      "professional skills ,\n",
      "entrepreneurial skills ,\n",
      "core executive leadership skills ,\n",
      "presentation skills ,\n",
      "core management skills ,\n",
      "soft skills ,\n",
      "--------------------------------\n",
      "             -- management --\n",
      "---------------------------------\n",
      "cost management ,\n",
      "management ,\n",
      "scope management ,\n",
      "stakeholder management ,\n",
      "procurement management ,\n",
      "integration management ,\n",
      "sales management ,\n",
      "human resources management ,\n",
      "time management ,\n",
      "brand management ,\n",
      "sales pipeline management ,\n",
      "strategic program management ,\n",
      "technology (for it project management) ,\n",
      "risk management ,\n",
      "executive management ,\n",
      "core management skills ,\n",
      "change management ,\n"
     ]
    }
   ],
   "source": [
    "print('----------------Technical skills---------------')\n",
    "for zzz in dore:\n",
    "    print(zzz)\n",
    "print()\n",
    "print('----------------Technical skills---------------')\n",
    "print()\n",
    "\n",
    "for i in my_final:\n",
    "    \n",
    "    print('--------------------------------')\n",
    "    print('             --', i , '--')\n",
    "    print('---------------------------------')\n",
    "    for j in my_final[i]:\n",
    "        print(j, ',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
